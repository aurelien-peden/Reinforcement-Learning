{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop him off in another. You receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\n",
    "https://gym.openai.com/envs/Taxi-v3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-table\n",
    "\n",
    "The Q-table is an array with each line corresponding to a specific state, and each column corresponding to a specific action.\n",
    "We can then refer to that Q-table to select a specific state-action pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "\n",
    "q_table = np.zeros((state_space_size, action_space_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [-0.98035354, -0.73010001, -3.16883054, -4.11774313,  9.6220697 ,\n",
       "        -7.45269128],\n",
       "       [ 2.80500793, -1.96587917, -2.03462204, -0.48524037, 14.11880599,\n",
       "        -5.39463893],\n",
       "       ...,\n",
       "       [-1.06604517,  0.2179915 , -0.99551198, -0.99337936, -3.51140357,\n",
       "        -4.1971362 ],\n",
       "       [-2.55285152, -2.57588094, -2.53587877, -2.6410169 , -2.84132105,\n",
       "        -6.23360338],\n",
       "       [-0.1       , -0.1       , -0.1       ,  1.96784961, -1.        ,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate and discount rate\n",
    "\n",
    "The learning rate decides how fast we should replace the previous q value from the q-table that we are updating. We don't want the q-value to be completely replaced with the new q-value.\n",
    "\n",
    "The discount rate $ \\gamma $ determines the present value of future rewards:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*} \n",
    "G_{t} &=&R_{t+1}+\\gamma R_{t+2}+\\gamma ^{2}R_{t+3}+\\cdots \\\\ &=&\\sum_{k=0}^{\\infty }\\gamma^{k}R_{t+k+1}\\text{.} \n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "With a smaller discount rate, the agent will care more about the immediate reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "max_steps_per_episode = 4000\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_all_episodes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration vs Exploitation\n",
    "\n",
    "To avoid exploiting actions that might not be as rewarding as others actions in the long run, we create an exploration rate threshold variable and an exploration rate variable. When the former one is smaller than the last one, we pick an action from the action space that might not be the one with the highest q value for now. \n",
    "\n",
    "The Q-value for the state-action pair in the Q-table is updated using the following formula:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "q^{new}\\left( s,a\\right) =\\left( 1-\\alpha \\right) ~\\underset{\\text{old value} }{\\underbrace{q\\left( s,a\\right)}\\rule[-0.05in]{0in}{0.2in} \\rule[-0.05in]{0in}{0.2in}\\rule[-0.1in]{0in}{0.3in}}+\\alpha \\overset{\\text{ learned value}}{\\overbrace{\\left(R_{t+1}+\\gamma \\max_{a^{^{\\prime }}}q\\left( s^{\\prime },a^{\\prime }\\right) \\right) }} \n",
    "\\end{equation*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 10000/10000 [00:02<00:00, 4410.13it/s]\n"
     ]
    }
   ],
   "source": [
    "for episode in tqdm(range(num_episodes)):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        q_table[state, action] = (1 - learning_rate) * q_table[state, action] + learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "        \n",
    "        state = new_state\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "    \n",
    "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    rewards_all_episodes.append(rewards_current_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  -15.946999999999774\n",
      "2000 :  7.445999999999958\n",
      "3000 :  7.34299999999996\n",
      "4000 :  7.441999999999964\n",
      "5000 :  7.485999999999968\n",
      "6000 :  7.378999999999959\n",
      "7000 :  7.511999999999966\n",
      "8000 :  7.388999999999959\n",
      "9000 :  7.471999999999961\n",
      "10000 :  7.375999999999965\n"
     ]
    }
   ],
   "source": [
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********Q-table********\n",
      "\n",
      "[[ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 2.46810726  1.02853031  1.69518577 -2.85338392  9.6220697  -4.18283343]\n",
      " [ 9.01010468  4.15363232  6.82146892  9.55553998 14.11880599  1.03650442]\n",
      " ...\n",
      " [-0.78295732  0.84603199 -0.83151088  5.55285716 -4.51046037 -4.1971362 ]\n",
      " [-2.03272021 -0.33638664 -2.49883597  2.83428751 -4.30641753 -7.52622502]\n",
      " [ 1.28638332  1.31172739 -0.1        12.27888007 -1.86117362 -1.15707714]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n********Q-table********\\n\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "****Successful dropoff!****\n"
     ]
    }
   ],
   "source": [
    "for episode in range(3):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        action = np.argmax(q_table[state,:])        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 20:\n",
    "                print(\"****Successful dropoff!****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"****Fail!****\")\n",
    "                time.sleep(3)\n",
    "                clear_output(wait=True)\n",
    "            break\n",
    "        \n",
    "        state = new_state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
